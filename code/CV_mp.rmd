---
title: "CV_mp"
author: "Yiming Chen"
date: "2025-10-01"
output:
  pdf_document: default
  html_document: default
---

```{r}
# load packages
library(GDINA)
library(mirt)
library(mvtnorm)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
```

# Split simualted item response data into k-folds for cross-validation
Each entry is designed to have missing entries reserved for test set.

## A function to randomly assign fold numbers for each row (individual)
```{r}
gen_data_for_row <- function(J,nfold){

  # Number of complete repeats per fold
  base_repeats <- floor(J / nfold)
  remaining <- J %% nfold

  # Create deterministic part
  folds <- rep(1:nfold, times = base_repeats)

  # Add randomly selected leftover folds
  if (remaining > 0) {
    folds <- c(folds, sample(1:nfold, size = remaining))
  }

  # Shuffle the fold assignments for this row
  sample(folds, size = J)
}
```

```{r}
# example use case
# each run time produces different results -> should set.seed()
gen_data_for_row(30, 5)
gen_data_for_row(5, 5)
```

## A function to split item response data into k-folds 
```{r}
split_data <- function(data,kfold=5,prop=NULL){
  if(!is.matrix(data)){
    data <- as.matrix(data)
  }

  N <- nrow(data)
  J <- ncol(data)
  ind <- matrix(NA,nrow = N,ncol = J)
  for(i in 1:N){
    ind[i,] <- gen_data_for_row(J,kfold)
  }
  newdata <- list()
  for(n in 1:kfold){
    kfold.loc <- which(ind==n)
    temp <- data
    temp[kfold.loc] <- NA
    newdata[[n]] <- as.data.frame(temp)
  }

  ret <- list(cvdata=newdata,cvfold=ind,kfold=kfold,rawdata=data)
}
```

# Cross Validated Congitive Diagnostic Model Fitting
```{r}
cv_fit_cdm_mp <- function(cvdata, Q, estimator="mp", ...){
  N <- nrow(cvdata$rawdata)
  J <- ncol(cvdata$rawdata)
  K <- ncol(Q)
  ind <- cvdata$cvfold

  metrics <- fit <- obs_pred_p <- train_metrics <- train_obs_pred_p <- list()
  for(n in 1:cvdata$kfold){
    nfold.loc <- which(ind==n)
    train.loc <- which(ind!=n)
    nfold.length <- length(nfold.loc)
    train.length <- length(train.loc)
    
    # Fit model
    fit[[n]] <- GDINA(cvdata$cvdata[[n]],Q, model = "LLM", verbose = 0)

    # Validation sample metrics
    # profiles <- as.matrix(personparm(fit[[n]],what=estimator)[,1:K])
    # gr <- GDINA:::matchMatrix(as.matrix(attributepattern(K)),profiles)
    # cdmp <- t(fit[[n]]$LC.prob[,gr])
    # p <- cdmp[nfold.loc]
    # pred.data <- rbinom(nfold.length,1,p)
    
    person_parm <- personparm(fit[[n]], what = estimator)
    item_parm <- coef(fit[[n]], what = "delta")
    pred_p <- NULL
    
    for (j in 1:J) {
      qj <- Q[j, ]
      pred_p <- cbind(pred_p, plogis(cbind(1, person_parm[, which(qj == 1), drop = FALSE]) %*% item_parm[[j]]))
    }
    
    pred_Y <- 1 * (pred_p > matrix(runif(N * J), nrow = N, ncol = J))
    
    p <- pred_p[nfold.loc]
    pred.data <- pred_Y[nfold.loc]
    
    obs.data <- cvdata$rawdata[nfold.loc]
    opp <- data.frame(obs.data, pred.data, p)
    obs_pred_p[[n]] <- opp[complete.cases(opp), ]
    metrics[[n]] <- classification_metrics(obs_pred_p[[n]]$obs.data, 
                                           obs_pred_p[[n]]$pred.data, 
                                           obs_pred_p[[n]]$p)
    
    # Training sample metrics
    
    #train_p <- cdmp[train.loc]
    #train_pred.data <- rbinom(train.length,1,train_p)
    
    train_p <- pred_p[train.loc]
    train_pred.data <- pred_Y[train.loc]
    
    train_obs.data <- cvdata$rawdata[train.loc]
    train_opp <- data.frame(train_obs.data, train_pred.data, train_p)
    train_obs_pred_p[[n]] <- train_opp[complete.cases(train_opp), ]
    train_metrics[[n]] <- classification_metrics(train_obs_pred_p[[n]]$train_obs.data, 
                                                 train_obs_pred_p[[n]]$train_pred.data, 
                                                 train_obs_pred_p[[n]]$train_p)
  }

  output <- combine_metrics_table(metrics)
  train_output <- combine_metrics_table(train_metrics)
  
  list(
    metrics = output,
    obs_pred_p = obs_pred_p,
    metrics_kfold = metrics,
    train_metrics = train_output,
    train_obs_pred_p = train_obs_pred_p,
    train_metrics_kfold = train_metrics
  )
}
```

```{r}
cv_fit_cdm_attr <- function(cvdata,Q,estimator="MAP",...){
  N <- nrow(cvdata$rawdata)
  J <- ncol(cvdata$rawdata)
  K <- ncol(Q)
  ind <- cvdata$cvfold

  metrics <- fit <- obs_pred_p <- train_metrics <- train_obs_pred_p <- list()
  for(n in 1:cvdata$kfold){
    nfold.loc <- which(ind==n)
    train.loc <- which(ind!=n)
    nfold.length <- length(nfold.loc)
    train.length <- length(train.loc)

    # Fit model
    fit[[n]] <- GDINA(cvdata$cvdata[[n]],Q,...)

    # Validation sample metrics
    profiles <- as.matrix(personparm(fit[[n]],what=estimator)[,1:K])
    gr <- GDINA:::matchMatrix(as.matrix(attributepattern(K)),profiles)
    cdmp <- t(fit[[n]]$LC.prob[,gr])
    p <- cdmp[nfold.loc]
    pred.data <- rbinom(nfold.length,1,p)
    obs.data <- cvdata$rawdata[nfold.loc]
    opp <- data.frame(obs.data,pred.data,p)
    obs_pred_p[[n]] <- opp[complete.cases(opp),]
    metrics[[n]] <- classification_metrics(obs_pred_p[[n]]$obs.data,
                                           obs_pred_p[[n]]$pred.data,
                                           obs_pred_p[[n]]$p)

    # Training sample metrics
    train_p <- cdmp[train.loc]
    train_pred.data <- rbinom(train.length,1,train_p)
    train_obs.data <- cvdata$rawdata[train.loc]
    train_opp <- data.frame(train_obs.data,train_pred.data,train_p)
    train_obs_pred_p[[n]] <- train_opp[complete.cases(train_opp),]
    train_metrics[[n]] <- classification_metrics(train_obs_pred_p[[n]]$train_obs.data,
                                                 train_obs_pred_p[[n]]$train_pred.data,
                                                 train_obs_pred_p[[n]]$train_p)
  }

  output <- combine_metrics_table(metrics)
  train_output <- combine_metrics_table(train_metrics)

  list(
    metrics = output,
    obs_pred_p = obs_pred_p,
    metrics_kfold = metrics,
    train_metrics = train_output,
    train_obs_pred_p = train_obs_pred_p,
    train_metrics_kfold = train_metrics
  )
}
```

```{r}
cv_fit_irt <- function(cvdata, model_specification, estimator="MAP",...){
  N <- nrow(cvdata$rawdata)
  J <- ncol(cvdata$rawdata)
  ind <- cvdata$cvfold

  metrics <- fit <- obs_pred_p <- train_metrics <- train_obs_pred_p <- list()
  for(n in 1:cvdata$kfold){
    p <- matrix(NA,N,J)
    kfold.loc <- which(ind==n)
    train.loc <- which(ind!=n)
    kfold.length <- length(kfold.loc)
    train.length <- length(train.loc)
    
    # Fit model
    # Warning: MHRM terminated after 2000 iterations. --> increase iterations to 5000?
    # mirt(,technical = list(NCYCLES = 5000))
    fit[[n]] <- mirt::mirt(cvdata$cvdata[[n]], model = model_specification, method = "MHRM", ...)
    
    # Validation sample metrics
    theta <- mirt::fscores(fit[[n]],method=estimator, QMC = TRUE)
    for(j in 1:J){
      p[,j] <- mirt::expected.item(mirt::extract.item(fit[[n]],item = j),theta) 
    }
    vp <- p[kfold.loc]
    pred.data <- rbinom(kfold.length,1,vp)
    obs.data <- cvdata$rawdata[kfold.loc]
    opp <- data.frame(obs.data,pred.data,vp)
    obs_pred_p[[n]] <- opp[complete.cases(opp),]
    metrics[[n]] <- classification_metrics(obs_pred_p[[n]]$obs.data,
                                           obs_pred_p[[n]]$pred.data,
                                           obs_pred_p[[n]]$vp)
    
    # Training sample metrics
    train_p <- p[train.loc]
    train_pred.data <- rbinom(train.length,1,train_p)
    train_obs.data <- cvdata$rawdata[train.loc]
    train_opp <- data.frame(train_obs.data,train_pred.data,train_p)
    train_obs_pred_p[[n]] <- train_opp[complete.cases(train_opp),]
    train_metrics[[n]] <- classification_metrics(train_obs_pred_p[[n]]$train_obs.data,
                                                 train_obs_pred_p[[n]]$train_pred.data,
                                                 train_obs_pred_p[[n]]$train_p)
  }

  output <- combine_metrics_table(metrics)
  train_output <- combine_metrics_table(train_metrics)
  
  list(
    metrics = output,
    obs_pred_p = obs_pred_p,
    metrics_kfold = metrics,
    train_metrics = train_output,
    train_obs_pred_p = train_obs_pred_p,
    train_metrics_kfold = train_metrics
  )
}
```

```{r}
classification_metrics <- function(observed, predicted, p) {
  # Ensure input vectors are numeric
  observed <- as.numeric(as.character(observed))
  predicted <- as.numeric(as.character(predicted))
  p <- as.numeric(p)

  # Confusion matrix components
  tp <- sum(predicted == 1 & observed == 1)
  tn <- sum(predicted == 0 & observed == 0)
  fp <- sum(predicted == 1 & observed == 0)
  fn <- sum(predicted == 0 & observed == 1)
  cat("\nTP=", tp, " TN=", tn, " FP=", fp, "FN=", fn)
  
  # Basic metrics
  accuracy <- mean(predicted == observed)
  precision <- if ((tp + fp) == 0) NA else tp / (tp + fp) # PPV 
  recall <- if ((tp + fn) == 0) NA else tp / (tp + fn) # sensitivity
  specificity <- if ((tn + fp) == 0) NA else tn / (tn + fp)
  f1_score <- if (is.na(precision) || is.na(recall) || (precision + recall == 0)) NA else 2 * precision * recall / (precision + recall)

  # MCC
  mcc_denom <- sqrt(
    as.numeric((tp + fp)) *
    as.numeric((tp + fn)) *
    as.numeric((tn + fp)) *
    as.numeric((tn + fn))
  )
  mcc <- if (mcc_denom == 0) NA else ((tp * tn) - (fp * fn)) / mcc_denom

  # AUC - TPR (recall, Y-axis) vs. FPR (X-axis) Curve
  auc_val <- tryCatch({
    pROC::auc(pROC::roc(observed, p))
  }, error = function(e) NA)

  # PR AUC - Precision (Y-axis) - Recall (X-axis) Curve
  pr_auc_val <- tryCatch({
    fg <- p[observed == 1] # foreground
    bg <- p[observed == 0] # background
    # class0 belongs to the positive class -> observed == 1 -> fg
    # class1 belongs to the negative class -> observed == 0 -> bg
    pr <- PRROC::pr.curve(scores.class0 = fg, scores.class1 = bg)
    pr$auc.integral
  }, error = function(e) NA)

  # Log Loss
  log_loss <- {
    eps <- 1e-15
    p <- pmin(pmax(p, eps), 1 - eps)
    -mean(observed * log(p) + (1 - observed) * log(1 - p))
  }

  # Confusion matrix
  confusion_matrix <- matrix(c(tn, fp, fn, tp), nrow = 2, byrow = TRUE,
                             dimnames = list("Predicted" = c("0", "1"),
                                             "Actual" = c("0", "1")))

  # Return all
  return(list(
    accuracy = accuracy,
    precision = precision,
    recall = recall,
    specificity = specificity,
    f1_score = f1_score,
    mcc = mcc,
    auc = auc_val,
    pr_auc = pr_auc_val,
    log_loss = log_loss,
    confusion_matrix = confusion_matrix
  ))
}
```

```{r}
combine_metrics_table <- function(metrics_list,digits=3) {
  # Extract all scalar metric names (exclude confusion_matrix)
  metric_names <- setdiff(names(metrics_list[[1]]), "confusion_matrix")

  # Convert each list element to a one-row data frame
  metrics_df <- do.call(rbind, lapply(metrics_list, function(m) {
    as.data.frame(as.list(m[metric_names]))
  }))

  # Compute mean for each metric
  average_row <- colMeans(metrics_df, na.rm = TRUE)

  # Add the average as the final row
  metrics_df <- rbind(metrics_df, Average = average_row)

  metrics_df <- round(metrics_df, digits)
  # Set row names: Fold_1, Fold_2, ..., Average
  rownames(metrics_df) <- c(paste0("Fold_", seq_len(nrow(metrics_df) - 1)), "Average")

  return(metrics_df)
}
```

## Define a function to run cross validation
```{r}
Q_complex_3 <- matrix(c(
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,1,1, 1,1,1, 1,1,1,
            1,1,1, 1,1,1, 1,1,1
        ), ncol = 3, byrow = TRUE)

# for complex structure: function to align model specification to Q-matrix
to_model_string <- function(Q){
  factors <- lapply(seq_len(ncol(Q)), function(f){
    items <- which(Q[, f] == 1)
    paste0("F", f, " = ", paste(items, collapse = ","))
  })
  cov <- paste0("COV = ", paste0("F", 1:ncol(Q), collapse = "*"))
  paste(c(factors, cov), collapse = "\n")
}

# define Q-matrix for different conditions and then run_cv_fit functions
run_cv_fit <- function(cvobj, name) {
  str <- if (grepl("Simple", name)) "Simple" else "Complex"
  nd <- as.numeric(sub(".*dim([0-9]+)_.*", "\\1", name))
  
  if (str == "Simple") {
    block_size <- 30 / nd
    Q <- matrix(0, nrow = 30, ncol = nd)
    for (i in 1:nd) {
      start_index <- (i - 1) * block_size + 1
      end_index <- i * block_size
      Q[start_index:end_index, i] <- 1
    }
    factor_lines <- vapply(1:nd, function(i){
      start_index <- (i - 1) * block_size + 1
      end_index <- i * block_size
      paste0("F", i, " = ", paste(start_index:end_index, collapse = ","))
    }, character(1))
    cov_line <- paste0("COV = ", paste0("F", 1:nd, collapse = "*"))
    model_specification <- paste(c(factor_lines, cov_line), collapse = "\n")
    
  } else if (str == "Complex" & nd == 3) {
    Q <- Q_complex_3
    model_specification <- to_model_string(Q)
    
  } else if (str == "Complex" & nd == 5) {
    Q <- sim30GDINA$simQ
    model_specification <- to_model_string(Q)
  }
  
  res_mp <- cv_fit_cdm_mp(cvobj, Q, model = "LLM"
                          #, mono.constraint = TRUE
                          )
  res_attribute <- cv_fit_cdm_attr(cvobj, Q, model = "LLM"
                                   #, mono.constraint = TRUE
                                   )
  res_irt <- cv_fit_irt(cvobj, model_specification)
  
  return(list(
    mp = res_mp, 
    attribute = res_attribute, 
    irt = res_irt))
}
```

# Apply split_data function to get CDM cvdata
```{r}
# set seed to ensure reproducibility in splitting data 
set.seed(101)

# read CDM simulated data and save them into a list called cvdata_cdm
files <- list.files("../CDM_sim/data", pattern = "^data_.*\\.csv$", full.names = TRUE)
names(files) <- basename(files)    

# cdm_data is a list of 120 data frames
cdm_data <- lapply(files, read.csv)
length(cdm_data)

# generate cvdata using split_data function
cvdata_cdm <- lapply(cdm_data, split_data, kfold = 5)

# access data name for each data
names(cvdata_cdm)[[1]]
```

```{r}
#cvdata$data_Complex_cor0_dim3_rep1.csv$cvfold[1:5,]
cvdata_cdm[[1]]$cvfold[1:5,]
names(cvdata_cdm)[1:10]
names(cvdata_cdm)[11:20]

#cvdata_cdm[[1]]$cvdata_cdm[[1]]
ind <- cvdata_cdm[[1]]$cvfold
nfold.loc <- which(ind==1)
train.loc <- which(ind != 1)
length(nfold.loc)
length(train.loc)
```

# Apply run_cv_fit function to CDM data
```{r, message=FALSE}
# ~ several hours
# set.seed(101)
# 
# simple_dim3 <- grep("Simple.*dim3", names(cvdata_cdm), value = TRUE)
# res_simple_dim3 <- mapply(run_cv_fit, cvobj = cvdata_cdm[simple_dim3], name = simple_dim3, SIMPLIFY = FALSE)
# 
# simple_dim5 <- grep("Simple.*dim5", names(cvdata_cdm), value = TRUE)
# res_simple_dim5 <- mapply(run_cv_fit, cvobj = cvdata_cdm[simple_dim5], name = simple_dim5, SIMPLIFY = FALSE)
# 
# complex_dim3 <- grep("Complex.*dim3", names(cvdata_cdm), value = TRUE)
# res_complex_dim3 <- mapply(run_cv_fit, cvobj = cvdata_cdm[complex_dim3], name = complex_dim3, SIMPLIFY = FALSE)
# 
# complex_dim5 <- grep("Complex.*dim5", names(cvdata_cdm), value = TRUE)
# res_complex_dim5 <- mapply(run_cv_fit, cvobj = cvdata_cdm[complex_dim5], name = complex_dim5, SIMPLIFY = FALSE)
```

Got two kinds of warning messages a lot of times:

1) Warning: The following factor score estimates failed to converge successfully: 17,19,23,56,82,219,223,228,271,288,322,339,348,360,370,378,395,417,435,463,502,559,573,610,634,646,652,663,671,708,731,738,744,768,770,797,824,846,847,849,924,951,1004,1009,1079,1104,1146,1147,1165,1170,1173,1226,1252,1258,1268,1297,1298,1300,1301,1309,1329,1370,1373,1409,1418,1421,1466,1513,1514,1516,1558,1593,1596,1641,1661,1685,1696,1702,1720,1729,1755,1775,1789,1790,1792,1800,1802,1852,1866,1882,1894,1896,1899,1943,1975,1983,1997 --> 

fscore() check theta estimation for these reported individuals

2) Warning: MHRM terminated after 2000 iterations. --> increase iterations to 5000?

```{r}
# ~ several minutes
# saveRDS(res_simple_dim3, "../CV_results/cdm_data_res_simple_dim3.rds")
# saveRDS(res_simple_dim5, "../CV_results/cdm_data_res_simple_dim5.rds")
# saveRDS(res_complex_dim3, "../CV_results/cdm_data_res_complex_dim3.rds")
# saveRDS(res_complex_dim5, "../CV_results/cdm_data_res_complex_dim5.rds")
```

```{r}
cdm_data_res_simple_dim3 <- readRDS("../CV_results/cdm_data_res_simple_dim3.rds")
```

```{r}
cdm_data_res_simple_dim3$data_Simple_cor0_dim3_rep1.csv$mp$metrics
cdm_data_res_simple_dim3$data_Simple_cor0_dim3_rep1.csv$attribute$metrics
cdm_data_res_simple_dim3$data_Simple_cor0_dim3_rep1.csv$irt$metrics
```

```{r}
cdm_data_res_simple_dim5 <- readRDS("../CV_results/cdm_data_res_simple_dim5.rds")
```

```{r}
cdm_data_res_simple_dim5$data_Simple_cor0_dim5_rep1.csv$mp$metrics
cdm_data_res_simple_dim5$data_Simple_cor0_dim5_rep1.csv$attribute$metrics
cdm_data_res_simple_dim5$data_Simple_cor0_dim5_rep1.csv$irt$metrics
```

```{r}
cdm_data_res_complex_dim3 <- readRDS("../CV_results/cdm_data_res_complex_dim3.rds")
```

```{r}
cdm_data_res_complex_dim3$data_Complex_cor0_dim3_rep1.csv$mp$metrics
cdm_data_res_complex_dim3$data_Complex_cor0_dim3_rep1.csv$attribute$metrics
cdm_data_res_complex_dim3$data_Complex_cor0_dim3_rep1.csv$irt$metrics
```

```{r}
cdm_data_res_complex_dim5 <- readRDS("../CV_results/cdm_data_res_complex_dim5.rds")
```

```{r}
cdm_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$mp$metrics
cdm_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$attribute$metrics
cdm_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$irt$metrics
```

```{r}
# get aggregated results
res_all <- c(cdm_data_res_simple_dim3, cdm_data_res_simple_dim5, cdm_data_res_complex_dim3, cdm_data_res_complex_dim5)

# dimension of metrics_all_cdm is 120 * 3 rows and 14 columns
# 12 conditions of each 3 methods each have 10 replications
metrics_all_cdm <- lapply(names(res_all), function(name) {
  res <- res_all[[name]]
  
  structure <- ifelse(str_detect(name, "Simple"), "Simple", "Complex")
  correlation <- as.numeric(str_extract(name, "(?<=cor)\\d+(\\.\\d+)?"))
  dim <- as.numeric(str_extract(name, "(?<=dim)\\d+"))
  replicate <- as.numeric(str_extract(name, "(?<=rep)\\d+"))
  
  methods <- c("mp", "attribute", "irt")
  
  bind_rows(lapply(methods, function(m) {
    data.frame(
      method = m,
      structure = structure,
      correlation = correlation,
      dim = dim,
      replicate = replicate,
      accuracy = res[[m]]$metrics$accuracy[6],
      precision = res[[m]]$metrics$precision[6],
      recall  = res[[m]]$metrics$recall[6],
      specificity  = res[[m]]$metrics$specificity[6],
      f1_score  = res[[m]]$metrics$f1_score[6],
      mcc = res[[m]]$metrics$mcc[6],
      auc  = res[[m]]$metrics$auc[6],
      pr_auc = res[[m]]$metrics$pr_auc[6],
      log_loss  = res[[m]]$metrics$log_loss[6]
    )
  }))
}) %>%
  bind_rows()

# group by str, cor, dim, and methods to get summarized metrics of 12 conditions
metrics_all_cdm <- metrics_all_cdm %>%
  # keep the order of three methods fixed like this
  mutate(method = factor(method, levels = c("mp", "attribute", "irt"))) %>% 
  group_by(structure, correlation, dim, method) %>%
  summarize(
    mean_accuracy = mean(accuracy),
    mean_precision = mean(precision),
    mean_recall = mean(recall),
    mean_specificity = mean(specificity),
    mean_f1_score = mean(f1_score),
    mean_mcc = mean(mcc),
    mean_auc = mean(auc),
    mean_pr_auc = mean(pr_auc),
    mean_log_loss = mean(log_loss),
    .groups = "drop"
    ) %>%
  arrange(structure, correlation, dim, method)
```

```{r}
metrics_all_cdm
write.csv(metrics_all_cdm, "../CV_results/metrics_results_cdm.csv", row.names = FALSE)
```

```{r}
# metrics_all_cdm
# names(res_all)
# access the individual data 
res_all[["data_Simple_cor0_dim3_rep1.csv"]]$mp$metrics$accuracy[6]
res_all[["data_Simple_cor0_dim5_rep1.csv"]]$mp$metrics$accuracy[6]
res_all[["data_Complex_cor0_dim3_rep1.csv"]]$mp$metrics$accuracy[6]
res_all[["data_Complex_cor0_dim5_rep1.csv"]]$mp$metrics$accuracy[6]
```

# Apply split_data function to get IRT cvdata
```{r}
# set seed to ensure reproducibility in splitting data 
set.seed(101)

# read IRT simulated data and save them into a list called cvdata_irt
files <- list.files("../IRT_sim/data", pattern = "^data_.*\\.csv$", full.names = TRUE)
names(files) <- basename(files)    

# irt_data is a list of 120 data frames
irt_data <- lapply(files, read.csv)
length(irt_data)

# generate cvdata using split_data function
cvdata_irt <- lapply(irt_data, split_data, kfold = 5)

# access data name for each data
names(cvdata_irt)[[1]]
```

# Apply run_cv_fit function to IRT data
```{r, message=FALSE}
# ~ several hours
# set.seed(101)

# simple_dim3 <- grep("Simple.*dim3", names(cvdata_irt), value = TRUE)
# res_simple_dim3 <- mapply(run_cv_fit, cvobj = cvdata_irt[simple_dim3], name = simple_dim3, SIMPLIFY = FALSE)
# 
# simple_dim5 <- grep("Simple.*dim5", names(cvdata_irt), value = TRUE)
# res_simple_dim5 <- mapply(run_cv_fit, cvobj = cvdata_irt[simple_dim5], name = simple_dim5, SIMPLIFY = FALSE)

# complex_dim3 <- grep("Complex.*dim3", names(cvdata_irt), value = TRUE)
# res_complex_dim3 <- mapply(run_cv_fit, cvobj = cvdata_irt[complex_dim3], name = complex_dim3, SIMPLIFY = FALSE)
# 
# complex_dim5 <- grep("Complex.*dim5", names(cvdata_irt), value = TRUE)
# res_complex_dim5 <- mapply(run_cv_fit, cvobj = cvdata_irt[complex_dim5], name = complex_dim5, SIMPLIFY = FALSE)
```

```{r}
# ~ several minutes
# saveRDS(res_simple_dim3, "../CV_results/irt_data_res_simple_dim3.rds")
# saveRDS(res_simple_dim5, "../CV_results/irt_data_res_simple_dim5.rds")
# saveRDS(res_complex_dim3, "../CV_results/irt_data_res_complex_dim3.rds")
# saveRDS(res_complex_dim5, "../CV_results/irt_data_res_complex_dim5.rds")
```

```{r}
irt_data_res_simple_dim3 <- readRDS("../CV_results/irt_data_res_simple_dim3.rds")
```

```{r}
irt_data_res_simple_dim3$data_Simple_cor0_dim3_rep1.csv$mp$metrics
irt_data_res_simple_dim3$data_Simple_cor0_dim3_rep1.csv$attribute$metrics
irt_data_res_simple_dim3$data_Simple_cor0_dim3_rep1.csv$irt$metrics
```

```{r}
irt_data_res_simple_dim5 <- readRDS("../CV_results/irt_data_res_simple_dim5.rds")
```

```{r}
irt_data_res_simple_dim5$data_Simple_cor0_dim5_rep1.csv$mp$metrics
irt_data_res_simple_dim5$data_Simple_cor0_dim5_rep1.csv$attribute$metrics
irt_data_res_simple_dim5$data_Simple_cor0_dim5_rep1.csv$irt$metrics
```

```{r}
# can also access train metrics, but they are not useful compared to test metrics
# irt_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$mp$metrics
# irt_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$mp$train_metrics
```

```{r}
irt_data_res_complex_dim3 <- readRDS("../CV_results/irt_data_res_complex_dim3.rds")
```

```{r}
irt_data_res_complex_dim3$data_Complex_cor0_dim3_rep1.csv$mp$metrics
irt_data_res_complex_dim3$data_Complex_cor0_dim3_rep1.csv$attribute$metrics
irt_data_res_complex_dim3$data_Complex_cor0_dim3_rep1.csv$irt$metrics
```

```{r}
irt_data_res_complex_dim5 <- readRDS("../CV_results/irt_data_res_complex_dim5.rds")
```

```{r}
irt_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$mp$metrics
irt_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$attribute$metrics
irt_data_res_complex_dim5$data_Complex_cor0_dim5_rep1.csv$irt$metrics
```

```{r}
# get aggregated results
res_all <- c(irt_data_res_simple_dim3, irt_data_res_simple_dim5, irt_data_res_complex_dim3, irt_data_res_complex_dim5)

# dimension of metrics_all_cdm is 120 * 3 rows and 14 columns
# 12 conditions of each 3 methods each have 10 replications
metrics_all_irt <- lapply(names(res_all), function(name) {
  res <- res_all[[name]]
  
  structure <- ifelse(str_detect(name, "Simple"), "Simple", "Complex")
  correlation <- as.numeric(str_extract(name, "(?<=cor)\\d+(\\.\\d+)?"))
  dim <- as.numeric(str_extract(name, "(?<=dim)\\d+"))
  replicate <- as.numeric(str_extract(name, "(?<=rep)\\d+"))
  
  methods <- c("mp", "attribute", "irt")
  
  bind_rows(lapply(methods, function(m) {
    data.frame(
      method = m,
      structure = structure,
      correlation = correlation,
      dim = dim,
      replicate = replicate,
      accuracy = res[[m]]$metrics$accuracy[6],
      precision = res[[m]]$metrics$precision[6],
      recall  = res[[m]]$metrics$recall[6],
      specificity  = res[[m]]$metrics$specificity[6],
      f1_score  = res[[m]]$metrics$f1_score[6],
      mcc = res[[m]]$metrics$mcc[6],
      auc  = res[[m]]$metrics$auc[6],
      pr_auc = res[[m]]$metrics$pr_auc[6],
      log_loss  = res[[m]]$metrics$log_loss[6]
    )
  }))
}) %>%
  bind_rows()

# group by str, cor, dim, and methods to get summarized metrics of 12 conditions
metrics_all_irt <- metrics_all_irt %>%
  # keep the order of three methods fixed like this
  mutate(method = factor(method, levels = c("mp", "attribute", "irt"))) %>% 
  group_by(structure, correlation, dim, method) %>%
  summarize(
    mean_accuracy = mean(accuracy),
    mean_precision = mean(precision),
    mean_recall = mean(recall),
    mean_specificity = mean(specificity),
    mean_f1_score = mean(f1_score),
    mean_mcc = mean(mcc),
    mean_auc = mean(auc),
    mean_pr_auc = mean(pr_auc),
    mean_log_loss = mean(log_loss),
    .groups = "drop"
    ) %>%
  arrange(structure, correlation, dim, method)
```

```{r}
metrics_all_irt
write.csv(metrics_all_irt, "../CV_results/metrics_results_irt.csv", row.names = FALSE)
```


# Access all 12 conditions from CDM & IRT to obtain three methods (mp, attribute, theta) with test set metrics (accuracy, mcc, log loss etc.) for each of 10 replications

```{r}
# cdm_data_simple_dim3 <- readRDS("../CV_results/cdm_data_res_simple_dim3.rds")
# cdm_data_simple_dim5 <- readRDS("../CV_results/cdm_data_res_simple_dim5.rds")
# cdm_data_complex_dim3 <- readRDS("../CV_results/cdm_data_res_complex_dim3.rds")
# cdm_data_complex_dim5 <- readRDS("../CV_results/cdm_data_res_complex_dim5.rds")
# 
# irt_data_simple_dim3 <- readRDS("../CV_results/irt_data_res_simple_dim3.rds")
# irt_data_simple_dim5 <- readRDS("../CV_results/irt_data_res_simple_dim5.rds")
# irt_data_complex_dim3 <- readRDS("../CV_results/irt_data_res_complex_dim3.rds")
# irt_data_complex_dim5 <- readRDS("../CV_results/irt_data_res_complex_dim5.rds")
```

```{r}
st <- "complex"
toupper(substr(st,1,1))
substring(st,2)
```

```{r}
# parameters
# model_types <- c("cdm", "irt")
# structures  <- c("simple", "complex")
# dims        <- c(3, 5)
# cors        <- c("0", "0.3", "0.6")
# reps        <- 10

model_types <- c("cdm", "irt")
structures  <- "complex"
dims        <- 5
cors        <- "0.3"
reps        <- 10

# metrics to extract
cv_metrics <- c("accuracy", "precision", "recall", "specificity",
                "f1_score", "mcc", "auc", "pr_auc", "log_loss")

all_results <- list()

counter <- 1

for (mt in model_types) {
  for (st in structures) {
    for (dm in dims) {
      for (cr in cors) {
        
        # build data name, e.g. "cdm_data_complex_dim5_cor0.3"
        obj_name <- paste0(mt, "_data_", st, "_dim", dm, "_cor", cr)
        
        # retrieve data name
        data_list <- get(obj_name)
        
        message("Processing: ", obj_name)
        
        # store results across 10 reps for current dataset
        results_config <- list()

        for (i in 1:reps) {
          
          # build dataset name, e.g. "data_Complex_cor0.3_dim5_rep3.csv"
          dataset_name <- paste0("data_",
                                 toupper(substr(st,1,1)), substring(st,2), # "Complex"
                                 "_cor", cr,
                                 "_dim", dm,
                                 "_rep", i, ".csv")
          
          # retrieve data name
          data_i <- data_list[[dataset_name]]
          
          # extract averaged metrics values from 5 folds by [6]
          metrics_values <- t(sapply(cv_metrics, function(m) {
            c(
              mp        = data_i$mp$metrics[[m]][6],
              attribute = data_i$attribute$metrics[[m]][6],
              theta     = data_i$irt$metrics[[m]][6]
            )
          }))
          
          df_i <- data.frame(
            model_type = mt,
            structure  = st,
            dim        = dm,
            correlation = cr,
            rep        = i,
            metric     = rownames(metrics_values),
            metrics_values,
            row.names = NULL
          )
          
          results_config[[i]] <- df_i
        }
        
        # bind 10 reps for current data set
        all_results[[counter]] <- bind_rows(results_config)
        counter <- counter + 1
      }
    }
  }
}

# combine all results by pivoting three methods
cv_metric_results <- bind_rows(all_results) %>%
  pivot_longer(cols = c(mp, attribute, theta),
               names_to = "method",
               values_to = "value") %>%
  pivot_wider(names_from = metric, values_from = value)

cv_metric_results
```

# Single condition results interpretation

CDM Data
```{r}
cdm_data_complex_dim5 <- readRDS("../CV_results/cdm_data_res_complex_dim5.rds")
```

```{r}
# str(cdm_data_res_complex_dim5_cor0.3$data_Complex_cor0.3_dim5_rep1.csv)
# cdm_data_res_complex_dim5_cor0.3[["data_Complex_cor0.3_dim5_rep1.csv"]]$mp$metrics$accuracy[6]
# cdm_data_res_complex_dim5_cor0.3$data_Complex_cor0.3_dim5_rep1.csv$mp$metrics$accuracy[6]
```

```{r}
# focus on one single condition of complex structure, dim of 5 and correlation of 0.3
cdm_data_complex_dim5_cor0.3 <- cdm_data_complex_dim5[grep("cor0\\.3", names(cdm_data_complex_dim5))]

# list all possible metrics derived from cross validation
cv_metrics <- c("accuracy", "precision", "recall", "specificity", "f1_score", "mcc", "auc", "pr_auc", "log_loss")

results_complex_dim5_cor0.3 <- list()

for (i in 1:10) {
  # construct the name of each data set
  data_name <- paste0("data_Complex_cor0.3_dim5_rep", i, ".csv")
  
  # extract the list of output for each data set
  data_i <- cdm_data_complex_dim5_cor0.3[[data_name]]
  
  # extract averaged metrics values from 5 folds by [6]
  metrics_values <- t(sapply(cv_metrics, function(m) {
    c(
      mp = data_i$mp$metrics[[m]][6],
      attribute = data_i$attribute$metrics[[m]][6],
      theta = data_i$irt$metrics[[m]][[6]]
    )
  }))
  
  df_i <- data.frame(rep = i, metric = rownames(metrics_values), metrics_values, row.names = NULL)
  results_complex_dim5_cor0.3[[i]] <- df_i
}

cdm_results_complex_dim5_cor0.3 <- bind_rows(results_complex_dim5_cor0.3) %>%
  pivot_longer(cols = c(mp, attribute, theta), 
               names_to = "method", 
               values_to = "value") %>%
  pivot_wider(names_from = metric, 
              values_from = value)

cdm_results_complex_dim5_cor0.3 
```

IRT data
```{r}
irt_data_complex_dim5 <- readRDS("../CV_results/irt_data_res_complex_dim5.rds")
```

```{r}
# focus on one single condition of complex structure, dim of 5 and correlation of 0.3
irt_data_complex_dim5_cor0.3 <- irt_data_complex_dim5[grep("cor0\\.3", names(irt_data_complex_dim5))]

# list all possible metrics derived from cross validation
cv_metrics <- c("accuracy", "precision", "recall", "specificity", "f1_score", "mcc", "auc", "pr_auc", "log_loss")

results_complex_dim5_cor0.3 <- list()

for (i in 1:10) {
  # construct the name of each data set
  data_name <- paste0("data_Complex_cor0.3_dim5_rep", i, ".csv")
  
  # extract the list of output for each data set
  data_i <- irt_data_complex_dim5_cor0.3[[data_name]]
  
  # extract averaged metrics values from 5 folds by [6]
  metrics_values <- t(sapply(cv_metrics, function(m) {
    c(
      mp = data_i$mp$metrics[[m]][6],
      attribute = data_i$attribute$metrics[[m]][6],
      theta = data_i$irt$metrics[[m]][[6]]
    )
  }))
  
  df_i <- data.frame(rep = i, metric = rownames(metrics_values), metrics_values, row.names = NULL)
  results_complex_dim5_cor0.3[[i]] <- df_i
}

irt_results_complex_dim5_cor0.3 <- bind_rows(results_complex_dim5_cor0.3) %>%
  pivot_longer(cols = c(mp, attribute, theta), 
               names_to = "method", 
               values_to = "value") %>%
  pivot_wider(names_from = metric, 
              values_from = value)

irt_results_complex_dim5_cor0.3 
```

Create plots to examine 
```{r}
# combine two results and add a unique identifier, source, to know what is the true data generating model
cdm_results_complex_dim5_cor0.3$source <- "CDM"
irt_results_complex_dim5_cor0.3$source <- "IRT"

combine_results_complex_dim5_cor0.3 <- rbind(cdm_results_complex_dim5_cor0.3, irt_results_complex_dim5_cor0.3)
combine_results_complex_dim5_cor0.3
```

```{r}
# write.csv(combine_results_complex_dim5_cor0.3, "combine_results_complex_dim5_cor0.3.csv", row.names = FALSE)
```

```{r}
combine_mp_attr <- combine_results_complex_dim5_cor0.3 %>%
  filter(method == "mp"| method == "attribute")

mp_attr_diff <- combine_mp_attr %>%
  select(rep, method, accuracy, mcc, log_loss, source) %>%
  group_by(source) %>%
  pivot_wider(names_from = method, values_from = c(accuracy, mcc, log_loss)) %>%
  mutate(
    accuracy_diff = accuracy_attribute - accuracy_mp,
    mcc_diff = mcc_attribute - mcc_mp,
    log_loss_diff = log_loss_attribute - log_loss_mp) %>%
  ungroup()

combine_mp_attr 
mp_attr_diff

ggplot(mp_attr_diff, aes(x = rep, y = accuracy_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "Accuracy Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("Accuracy Difference") +
  ggtitle("Accuracy Difference by Attribute - MP") +
  theme_bw()

ggplot(mp_attr_diff, aes(x = rep, y = mcc_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "MCC Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("MCC Difference") +
  ggtitle("MCC Difference by Attribute - MP") +
  theme_bw()

ggplot(mp_attr_diff, aes(x = rep, y = log_loss_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "Log Loss Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("Log Loss Difference") +
  ggtitle("Log Loss Difference by Attribute - MP") +
  theme_bw()
```

```{r}
combine_mp_theta <- combine_results_complex_dim5_cor0.3 %>%
  filter(method == "mp"| method == "theta")

mp_theta_diff <- combine_mp_theta %>%
  select(rep, method, accuracy, mcc, log_loss, source) %>%
  group_by(source) %>%
  pivot_wider(names_from = method, values_from = c(accuracy, mcc, log_loss)) %>%
  mutate(
    accuracy_diff = accuracy_mp - accuracy_theta,
    mcc_diff = mcc_mp - mcc_theta,
    log_loss_diff = log_loss_mp - log_loss_theta) %>%
  ungroup()

combine_mp_theta
mp_theta_diff

ggplot(mp_theta_diff, aes(x = rep, y = accuracy_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "Accuracy Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("Accuracy Difference") +
  ggtitle("Accuracy Difference by MP - Theta") +
  theme_bw()

ggplot(mp_theta_diff, aes(x = rep, y = mcc_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "MCC Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("MCC Difference") +
  ggtitle("MCC Difference by MP - Theta") +
  theme_bw()

ggplot(mp_theta_diff, aes(x = rep, y = log_loss_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "Log Loss Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("Log Loss Difference") +
  ggtitle("Log Loss Difference by MP - Theta") +
  theme_bw()
```

```{r}
combine_attr_theta <- combine_results_complex_dim5_cor0.3 %>%
  filter(method == "attribute"| method == "theta")

attr_theta_diff <- combine_attr_theta %>%
  select(rep, method, accuracy, mcc, log_loss, source) %>%
  group_by(source) %>%
  pivot_wider(names_from = method, values_from = c(accuracy, mcc, log_loss)) %>%
  mutate(
    accuracy_diff = accuracy_attribute - accuracy_theta,
    mcc_diff = mcc_attribute - mcc_theta,
    log_loss_diff = log_loss_attribute - log_loss_theta) %>%
  ungroup()

combine_attr_theta
attr_theta_diff

ggplot(attr_theta_diff, aes(x = rep, y = accuracy_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "Accuracy Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("Accuracy Difference") +
  ggtitle("Accuracy Difference by Attribute - Theta") +
  theme_bw()

ggplot(attr_theta_diff, aes(x = rep, y = mcc_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "MCC Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("MCC Difference") +
  ggtitle("MCC Difference by Attribute - Theta") +
  theme_bw()

ggplot(attr_theta_diff, aes(x = rep, y = log_loss_diff)) +
  geom_point(shape = 21, size = 2, aes(fill = source)) +
  geom_hline(yintercept = 0,
             color = "black", linetype = "dashed", linewidth = 0.8) +
  scale_fill_manual(
    name = "Log Loss Difference",
    values = c("CDM" = "red", "IRT" = "blue")
  ) +
  scale_x_continuous(name = "Replication", breaks = 1:10) +
  ylab("Log Loss Difference") +
  ggtitle("Log Loss Difference by Attribute - Theta") +
  theme_bw()
```


# Practice Section
Single try-out
```{r, message=FALSE}
set.seed(101)
Q_complex_3 <- matrix(c(
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,0,0, 0,1,0, 0,0,1,
            1,1,0, 1,0,1, 0,1,1,
            1,1,1, 1,1,1, 1,1,1,
            1,1,1, 1,1,1, 1,1,1
        ), ncol = 3, byrow = TRUE)

# cv1 <- cv_fit_cdm(cvdata[[1]], Q_complex_3, model = "LLM")
# cv1$metrics$accuracy[6]

res <- list()
res <- lapply(1:10, function(i) cv_fit_cdm(cvdata_cdm[[i]], Q_complex_3))
acc_try <- sapply(res, function(x) x$metrics$accuracy[6])
precision_try <- sapply(res, function(x) x$metrics$precision[6])

# get average of 10 replication
mean_acc_try <- mean(acc_try)
mean_precision_try <- mean(precision_try)
```

```{r}
#res[[1]]$metrics
```

```{r}
acc_try
mean_acc_try
precision_try
mean_precision_try
# accessing results for the second data
# res[[2]]
```


To understand and try code above
```{r}
set.seed(1000)

dat <- sim10GDINA$simdat
Q <- sim10GDINA$simQ

mod1 <- GDINA(dat = dat, Q = Q, model = "LLM")
#mod1

head(dat)
dim(dat) # 1000 rows, 10 columns

# binary mastery profile
# head(personparm(mod1))

# A1 A2 A3
# [1,]  1  0  1

# estimated mastery probability
pp <- personparm(mod1,what = "mp")
head(pp) # 1000 rows, 3 columns

#  A1     A2     A3
# [1,] 0.9967 0.1134 0.9780

# estimated item parameter delta
ip <- coef(mod1,what = "delta")
ip[[2]]

# d0      d1
# -2.1312  3.2737

N <- nrow(dat)
J <- ncol(dat)
pred.p <- NULL

for(j in 1:J){
  qj <- Q[j,]
  pred.p <- cbind(pred.p, plogis(cbind(1, pp[, which(qj == 1)]) %*% ip[[j]]))
}

dim(pred.p)
head(pred.p)

# make the uniform random draws a matrix same as the dimension of N * J
pred.Y <- 1 * (pred.p > matrix(runif(N * J), nrow = N, ncol = J))

dim(pred.Y)
head(pred.Y)
```

```{r}
Q[10,] # qj

#head(pp[,Q[2,]]) # can be wrong for items require multiple attributes, or require attributes other than A1

#which(Q[2,] == 1) # return the correct attribute number required for item 2
head(pp[,which(Q[2,] == 1), drop=FALSE]) # correct --> pp[, which(qj == 1)]

head(cbind(1,pp[,which(Q[2,] == 1), drop=FALSE])) # add 1 as a column 

#ip[[2]]
```

```{r}
dim(matrix(runif(N * J), nrow = N, ncol = J))
```

```{r, message=FALSE}
N <- nrow(cvdata[[1]]$rawdata)
J <- ncol(cvdata[[1]]$rawdata)
K <- ncol(Q_complex_3)
ind <- cvdata[[1]]$cvfold

metrics <- fit <- obs_pred_p <- train_metrics <- train_obs_pred_p <- list()
for(n in 1:cvdata[[1]]$kfold){
    nfold.loc <- which(ind==n)
    train.loc <- which(ind!=n)
    nfold.length <- length(nfold.loc)
    train.length <- length(train.loc)
    
    # Fit model
    fit[[n]] <- GDINA(cvdata[[1]]$cvdata[[n]], Q_complex_3, model = "LLM", verbose = 0)

    # Validation sample metrics
    person_parm <- personparm(fit[[n]], what = "mp")
    item_parm <- coef(fit[[n]], what = "delta")
    pred_p <- NULL
    
    for (j in 1:J) {
      qj <- Q_complex_3[j, ]
      pred_p <- cbind(pred_p, plogis(cbind(1, person_parm[, which(qj == 1), drop = FALSE]) %*% item_parm[[j]]))
    }
    
    pred_Y <- 1 * (pred_p > matrix(runif(N * J), nrow = N, ncol = J))
    
    p <- pred_p[nfold.loc]
    pred.data <- pred_Y[nfold.loc]
    
    obs.data <- cvdata[[1]]$rawdata[nfold.loc]
    opp <- data.frame(obs.data, pred.data, p)
    obs_pred_p[[n]] <- opp[complete.cases(opp),]
    metrics[[n]] <- classification_metrics(obs_pred_p[[n]]$obs.data, obs_pred_p[[n]]$pred.data, obs_pred_p[[n]]$p)
}
```

```{r}
obs_pred_p[[1]]
metrics[[1]]
```













